{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e360120-8890-49c5-b186-9a2d40d5e15e",
   "metadata": {},
   "source": [
    "# Segment Anything with FiftyOne\n",
    "In this notebook, you will use Segment Anything to segment images from a downloaded dataset and then examine the predicted segments using FiftyOne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dabf87-0cdb-41ef-a727-8d833c82b34d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Environment setup\n",
    "To begin, create a virtual environment using your tool of choice, and if following along in this notebook, make sure to run the following commands in your activated virtual environment to enable the virtual environment to be used by the notebook:\n",
    "\n",
    "```\n",
    "$ pip install ipykernel\n",
    "$ python -m ipykernel install --user --name=fiftyoneSAMenv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ce3193-5532-48d0-880a-1abf6326c85a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install fiftyone git+https://github.com/facebookresearch/segment-anything.git torch torchvision opencv-python numpy==1.24.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3087b86-74a5-4087-be15-6406abdd582e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import cv2\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import numpy as np\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9cef91-df48-4b54-968c-e6db09ec8eaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Segment Anything Setup\n",
    "After installing and importing the dependencies above, download the [default Segment Anything model checkpoint](https://github.com/facebookresearch/segment-anything?tab=readme-ov-file#model-checkpoints) to the same directory as this notebook.\n",
    "\n",
    "The cells below will load your downloaded SAM checkpoint into a model instance called `sam`, which will then be used to create a `SamPredictor` object. If you don't have a CUDA-enabled GPU, skip the third cell in this sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45dbdb4-2191-4289-8c24-526e1fd5be4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
    "model_type = \"vit_h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256d5ff8-76df-4e04-8e44-108e3e84f84c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b48d80-bbc1-4e41-a18c-b318225a3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this if you have a CUDA-enabled GPU\n",
    "sam.to(device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabfae92-9727-4119-9afc-30c2da355be2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb2ea2-7b6a-4b6c-89fa-4e9a077e3d6a",
   "metadata": {},
   "source": [
    "### Dataset Setup\n",
    "In the next cell, you use FiftyOne's data zoo to download the `quickstart` dataset. If you've previously downloaded it, the `load_zoo_dataset()` call will just load the dataset into memory from disk. Then, to conserve time, the next line will take a slice of 10 images from that dataset, creating a [DatasetView](https://docs.voxel51.com/user_guide/using_views.html) that you will use for the rest of this tutorial's operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6291a6ea-d6e1-4068-ba81-b3e67cc0a335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c901ccd-5b3b-4783-a064-730441eedba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = foz.load_zoo_dataset(\"quickstart\")\n",
    "sliced_view = dataset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d894b234-e8bc-4bfc-b26c-b9f637a01d5c",
   "metadata": {},
   "source": [
    "## Segment the Sliced DatasetView\n",
    "Next, you will use Segment Anything to generate segment masks for each image in the sliced dataset view. While Segment Anything can segment the entire image, you will use the ground truth bounding box labels in the dataset as prompts to target the segment.\n",
    "\n",
    "### Segmenting a Single Sample\n",
    "The collection of cells below will show you how to get segment masks for every detection in a single image sample, load them into the sample, and then view them in FiftyOne. After that, you'll learn how to put it all together for a whole dataset. First, grab the first image in the sliced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b5210-2fda-47a9-b741-e5db95411856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = sliced_view.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca843cb6-a546-4992-86bb-9edeea6dc714",
   "metadata": {},
   "source": [
    "Next, use OpenCV to open the image and change the color format from OpenCV's default BGR to RGB. The call to `set_image()` will generate embeddings for the `SamPredictor` to generate masks for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb354d-1a8f-448a-94c4-eeb1490ac6c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image = cv2.imread(sample[\"filepath\"])\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "predictor.set_image(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8179c6-fa9d-4fc6-b44a-6cbdbde1f867",
   "metadata": {},
   "source": [
    "This next cell is where the magic happens. In it, you will iterate through all of the ground truth detections in the sample image. For each detection, you will use a helper function in the `fiftyone.utils.voc` module to convert from FiftyOne's relative xywh format to an absolute xyxy format accepted by Segment Anything, and then use that as a prompt and generate a mask and score for that mask. Then, you'll make a copy of the ground truth detection and add the mask and its score to it. After the loop exits, you use the `predictions` list to construct a `Detections` object and add that to a new key in the sample called `predictions`.\n",
    "\n",
    "Note that you must do some transformations of the returned mask to render it properly in FiftyOne. First, you need to use `mask[0]` to get a 2-dimensional representation of the mask. Then, the mask needs to be trimmed to the size of the input box. Since the mask is a NumPy array, you can use list slicing to do that, with the slices being the range of the y coordinates and the range of the x coordinates.\n",
    "\n",
    "The very last step is to save the sample so that the new predictions can be loaded in FiftyOne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8b2b00-c7f4-4577-8e11-f456ef19df05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "h, w, _ = image.shape\n",
    "for detection in sample[\"ground_truth\"][\"detections\"]:\n",
    "    input_bbox = fo.utils.voc.VOCBoundingBox.from_detection_format(detection[\"bounding_box\"], (w, h))\n",
    "    mask, score, _ = predictor.predict(\n",
    "        box=np.array([input_bbox.xmin, input_bbox.ymin, input_bbox.xmax, input_bbox.ymax]),\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    prediction = deepcopy(detection)\n",
    "    prediction[\"mask\"] = mask[0][input_bbox.ymin:input_bbox.ymax+1, input_bbox.xmin:input_bbox.xmax+1]\n",
    "    prediction[\"confidence\"] = score\n",
    "    predictions.append(prediction)\n",
    "sample[\"predictions\"] = fo.Detections(detections=predictions)\n",
    "sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140680a0-1c27-474b-9fdb-424bab647b56",
   "metadata": {},
   "source": [
    "#### Start FiftyOne\n",
    "Next, start FiftyOne on the sliced dataset view you created earlier. Make sure the \"predictions\" checkbox is toggled on, and you should be able to see the instance segmentations detected by Segment Anything on the first image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4d335-bcd0-4ef2-a3bb-72cf10728945",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = fo.launch_app(sliced_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cae727c-39be-423f-9bdc-a9ab224c5091",
   "metadata": {},
   "source": [
    "### Segmenting the Dataset\n",
    "Now that the hard work is done, all that is left is to assemble the pieces you already implemented into another loop to generate segments for the whole dataset view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3610295-97b1-4df5-83d2-7dd5a3755a25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for sample in sliced_view:\n",
    "    image = cv2.imread(sample[\"filepath\"])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    predictor.set_image(image)\n",
    "    predictions = []\n",
    "    h, w, _ = image.shape\n",
    "    for detection in sample[\"ground_truth\"][\"detections\"]:\n",
    "        input_bbox = fo.utils.voc.VOCBoundingBox.from_detection_format(detection[\"bounding_box\"], (w, h))\n",
    "        mask, score, _ = predictor.predict(\n",
    "            box=np.array([input_bbox.xmin, input_bbox.ymin, input_bbox.xmax, input_bbox.ymax]),\n",
    "            multimask_output=False,\n",
    "        )\n",
    "        prediction = deepcopy(detection)\n",
    "        prediction[\"mask\"] = mask[0][input_bbox.ymin:input_bbox.ymax+1, input_bbox.xmin:input_bbox.xmax+1]\n",
    "        prediction[\"confidence\"] = score\n",
    "        predictions.append(prediction)\n",
    "    sample[\"predictions\"] = fo.Detections(detections=predictions)\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4824cc34-aa27-49ad-a046-4f239fc89c27",
   "metadata": {},
   "source": [
    "Now refresh the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db34231c-f195-40e2-973c-7ff5dae1a49a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739d9cb7-357c-4eec-8bb4-6e61df2a1c41",
   "metadata": {},
   "source": [
    "And that's it! In this tutorial you learned how to:\n",
    "- Load FiftyOne and Segment Anything\n",
    "- Use ground truth detections in a FiftyOne dataset to prompt Segment Anything for segmentation masks\n",
    "- Inspect those masks within the FiftyOne application"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "fiftyone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
